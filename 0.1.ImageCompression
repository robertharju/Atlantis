The MIP Mapping refers to the process at which an image is compressed, and re-written to be passed
on and reprocessed by the next step in the process... ad infinitude. New GPUs can process the 3:1 ratio
at a faster, more efficient rate. And actually get closer to being a sharper image/resolution.

Possible change in compression algo? - Through the old methodologies, images were (are) processed
through “MIP Isotropic” filtering (downsizing), at only an equal rate.

Downsizing is (was), as follows:

          256 x 256 samples down to 128 X 128
          then 128 x 128 samples down to 64 x 64
          then 64 x 64 samles down to 32 x 32
          then etc; etc; etc;...
          

Through the new series of GPUs, it is possible to process images through “MIP Antisotripic” filtering
(downsizing), at an unequal rate.

Downsizing is(could be), as follows:

          256 x 256 can sample down to 256 x 128
          or / then 256 x 128 can sample down to
          128 x 64
          or / then 256 x 64 or 128 x 32
          or 256 x 32 or 256 x 16 etc; etc; etc;...


MIP Antisotripic filtering also has the advantage of being able to probe (reference) from / to any point
at different axis, while maintaining full “pixel quality”.

Old GPU architecture (pre 10xx / 5xx / rx) caused pixel loss when increasing the following sample
ration, from left to right:

          2:1 | 4:1 | 8:1 | 16:1


The new generation of GPU architecture has "solved" this by maintaining the image sharpness/quality,
even when increasing the probe sample rate frequency*.

      *of which, can be adjusted up or down and never loose quality. This is also known as "texel
      sampling". At this point, this is where "summed-tables" were previously, more effective / efficient.

A possible way to solve the compression issues, as well, due to jpeg / Run-Length coding being too
slow of a process to maintain speed of the probe sample increase / decrease. It needs to have a way to
predict the order of rendered access texels.

For this, resort to "crunch" style codec libraries. Doing this will allow "Rate-Distortion
Optimization" (RDO). The previous method at doing such would be to use Block Truncation
Coding. However, if it were to change to another style, it might allow that compression to occur with
no loss.

Doing such would be possible through AMBTC - "Absolute Moment Block Truncation Coding".
This method preserves the “first absolute moment” in which the library / code is compiled and passed,
along with the mean of the equation. It comes out with less MSE (Means Squared Error), if any at
all. Cutting the process time down on each block.



**Continued**

The following is an idea of possibly simplifying the compression method. Not replacing but, adding
another layer to, allowing faster compiling across network(s).

Absolute Moment Block Truncation Coding (AMBTC) is computationally “simpler” than Block
Truncation Coding (BTC), this is where the typical result is a lower Mean Squared Error (MSE).


Utilizing -
“... sub-blocks of 4×4 pixels gives a compression ratio of 4:1 assuming 8-bit integer values are used during transmission or
storage. Larger blocks allow greater compression ("a" and "b" values spread over more pixels) however quality also reduces
with the increase in block size due to the nature of the algorithm. “

Being able to change / deviate from the standard block size, into any combination of compressed
blocks, from / to any point, will allow exponential growth of the network.

          “Vector quantization(VQ) is a classical quantization technique from signal processing that allows the modeling
          of probability density functions by the distribution of prototype vectors. It was originally used for data
          compression. It works by dividing a large set of points (vectors) into groups having approximately the same
          number of points closest to them. Each group is represented by its centroid point, as in k-means and some other
          clustering algorithms.
          The density matching property of vector quantization is powerful, especially for identifying the density of large
          and high-dimensional data. Since data points are represented by the index of their closest centroid, commonly
          occurring data have low error, and rare data high error. This is why VQ is suitable for lossy data compression. It
          can also be used for lossy data correction and density estimation.
          Vector quantization is based on the competitive learning paradigm, so it is closely related to the self-organizing
          map model and to sparse coding models used in deep learning algorithms such as autoencoder.”
          “The main advantage of VQ in pattern recognition is its low computational burden when compared with other
          techniques such as dynamic time warping (DTW) and hidden Markov model (HMM). The main drawback when
          compared to DTW and HMM is that it does not take into account the temporal evolution of the signals (speech,
          signature, etc.) because all the vectors are mixed up. In order to overcome this problem a multi-section
          codebook approach has been proposed. The multi-section approach consists of modelling the signal with
          several sections (for instance, one codebook for the initial part, another one for the center and a last codebook
          for the ending part). “


-->>*Think Quicktime (RPZA / SMC) Codec libraries. Or even MPEG-4*<<--

          “It is desirable to use a cooling schedule to produce convergence: see Simulated annealing. Another (simpler)
          method is LBG which is based on Means.
          The algorithm can be iteratively updated with 'live' data, rather than by picking random points from a data set,
          but this will introduce some bias if the data are temporally correlated over many samples.”
          “Vector quantization is used for lossy data compression, lossy data correction, pattern recognition, density
          estimation and clustering.
          Lossy data correction, or prediction, is used to recover data missing from some dimensions. It is done by finding
          the nearest group with the data dimensions available, then predicting the result based on the values for the
          missing dimensions, assuming that they will have the same value as the group's centroid.
          For density estimation, the area/volume that is closer to a particular centroid than to any other is inversely
          proportional to the density (due to the density matching property of the algorithm).”
